{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例：使用框架训练图像分类模型\n",
    "\n",
    "利用框架提供的模块，我们可以十分方便地训练深度学习模型。以 Kaggle 图像数据集 <https://www.kaggle.com/datasets/asaniczka/mammals-image-classification-dataset-45-animals/data> 为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据集\n",
    "\n",
    "将数据下载并解压到 `data` 目录下：解压后的结构为\n",
    "\n",
    "```\n",
    "data/mammals/\n",
    "        |-  african_elephant/\n",
    "        |   |-  african_elephant-0001.jpg\n",
    "        |   |-  *\n",
    "        |\n",
    "        |-  alpaca/\n",
    "        |   |-  alpaca-0001.jpg\n",
    "        |   |-  *\n",
    "        |\n",
    "        *\n",
    "```\n",
    "\n",
    "由于数据集中并未划分训练集和测试集，我们需要通过下面的单元格对数据集进行划分。或者你可以采用仓库作者生成的划分方式：`data/mammals_split/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要重新划分数据集，请将本单元格的最后一行取消注释并运行\n",
    "\n",
    "from pathlib import Path \n",
    "import random \n",
    "\n",
    "# 获取所有的类别\n",
    "def split_dataset(root: str = \"data\", split_prob: float = 0.8, seed: int = 0) -> None:\n",
    "    r\"\"\" split the dataset\n",
    "    \n",
    "    root (pathlike): diretory where 'mammals/' lays.\n",
    "    split_prob (float): the ratio of train-set against test-set.\n",
    "    seed (int): random number generator seed.\n",
    "    \"\"\"\n",
    "    root_dir = Path(root)\n",
    "    labels = []\n",
    "    train = []\n",
    "    test = []\n",
    "    label_idx = 0\n",
    "    \n",
    "    # 固定随机种子\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 获取类名称并将图片分入训练或测试集\n",
    "    for sub_dir in (root_dir / \"mammals\").glob(\"*\"):\n",
    "        labels.append(sub_dir.stem)\n",
    "        for file_path in sub_dir.glob(\"*\"):\n",
    "            split = train if random.random() < split_prob else test\n",
    "            split.append((file_path.name, label_idx))\n",
    "        label_idx += 1\n",
    "\n",
    "    split_dir = root_dir / \"mammals_split\"\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 写入 labels.txt\n",
    "    with open(split_dir / \"labels.txt\", \"w\") as f:\n",
    "        for i, label in enumerate(labels):\n",
    "            f.write(label + \"\\n\")\n",
    "\n",
    "    # 写入 train.txt\n",
    "    with open(split_dir / \"train.txt\", \"w\") as f:\n",
    "        for file_name, label in train:\n",
    "            f.write(file_name + f\",{label}\\n\")\n",
    "\n",
    "    # 写入 test.txt\n",
    "    with open(split_dir / \"test.txt\", \"w\") as f:\n",
    "        for file_name, label in test:\n",
    "            f.write(file_name + f\",{label}\\n\")\n",
    "\n",
    "# 重新划分数据集时，请指定随机种子\n",
    "# split_dataset(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建数据集类：`src.modules.SizedDatast`，访问该数据集对象得到的数据类型为\n",
    "```\n",
    "{\n",
    "    \"image\": torch.Tensor,\n",
    "    \"label\": int\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from src.modules import SizedDataset\n",
    "from pathlib import Path\n",
    "from typing import Callable, cast\n",
    "from typing_extensions import TypedDict\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class Data(TypedDict):\n",
    "    image: torch.Tensor \n",
    "    label: int\n",
    "\n",
    "\n",
    "class MammalsDataset(SizedDataset):\n",
    "    r\"\"\" Dataset of mammals images\n",
    "    \n",
    "    root (pathlike): directory where 'mammals/' and 'mammals_split/' locate.\n",
    "    train (bool): whether load the train-dataset or the test-dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        root: str, \n",
    "        train: bool = True, \n",
    "        transforms: Callable =  transforms.Compose([\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.root: Path = Path(root).absolute()\n",
    "        self.train: bool = train\n",
    "        self.labels: list[str] = self.__read_labels()\n",
    "        self.data: list[dict] = self.__read_data()\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Data:\n",
    "        path = self.__image_dir / self.data[index][\"path\"]\n",
    "        label = self.data[index][\"label\"]\n",
    "        image = self.transforms(Image.open(path))\n",
    "        return {\n",
    "            \"image\": cast(torch.Tensor, image),\n",
    "            \"label\": label\n",
    "        }\n",
    "    \n",
    "    def get_image_label(self, index: int) -> dict:\n",
    "        path = self.__image_dir / self.data[index][\"path\"]\n",
    "        label = self.data[index][\"label\"]\n",
    "        image = Image.open(path)\n",
    "        return {\n",
    "            \"image\": image, \"label\": self.labels[label]\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def __image_dir(self): return self.root / \"mammals\"\n",
    "    \n",
    "    @property\n",
    "    def __split_dir(self): return self.root / \"mammals_split\"\n",
    "    \n",
    "    @property\n",
    "    def __label_file(self): return self.__split_dir / \"labels.txt\"\n",
    "    \n",
    "    @property\n",
    "    def __split_file(self): \n",
    "        name = \"train.txt\" if self.train else \"test.txt\"\n",
    "        return self.__split_dir / name \n",
    "    \n",
    "    def __read_labels(self) -> list[str]:\n",
    "        labels = []\n",
    "        with open(self.__label_file, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                labels.append(line.strip())\n",
    "        return labels\n",
    "    \n",
    "    def __read_data(self) -> list[dict]:\n",
    "        data = []\n",
    "        with open(self.__split_file, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                [name, label_index] = line.strip().split(\",\")\n",
    "                label_index = int(label_index)\n",
    "                dic: dict = {\n",
    "                    \"path\": self.labels[label_index] + \"/\" + name,\n",
    "                    \"label\": label_index\n",
    "                }\n",
    "                data.append(dic)\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展示数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "image_dataset = MammalsDataset(\"../data/\", transforms=lambda x: x )\n",
    "index = random.randint(0, len(image_dataset)-1)\n",
    "\n",
    "print(image_dataset.labels[image_dataset[index][\"label\"]])\n",
    "image_dataset[index][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 神经网络模型\n",
    "搭建 `src.modules.NeuralNetwork` 类。具体的，使用一个 `torchvision.models.resnet18` 实例并更换其分类头为 45 类，该分类头返回的是对应数据的 logits 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torchvision.models import resnet18, resnet34\n",
    "from src.modules import NeuralNetwork \n",
    "from typing import Literal\n",
    "\n",
    "class ResNet(NeuralNetwork):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        resnet: Literal[\"resnet18\", \"resnet34\"] = \"resnet18\", \n",
    "        use_pretrained: bool = False,\n",
    "        freeze_backbone: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.use_pretrained = use_pretrained\n",
    "        if resnet == \"resnet18\":\n",
    "            self.resnet = resnet18(pretrained=use_pretrained)\n",
    "        elif resnet == \"resnet34\":\n",
    "            self.resnet = resnet34(pretrained=use_pretrained)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for params in self.backbone_parameters():\n",
    "                params.requires_grad = False \n",
    "        self.resnet.fc = Linear(self.resnet.fc.in_features, 45, bias=True)\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "    \n",
    "    def forward(self, input_: torch.Tensor) ->torch.Tensor:\n",
    "        return self.resnet(input_)\n",
    "    \n",
    "    def backbone_parameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"fc\" not in name:\n",
    "                yield param\n",
    "    \n",
    "    def fc_parameters(self):\n",
    "        return self.resnet.fc.parameters()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        if self.use_pretrained:\n",
    "            self.resnet.fc.reset_parameters()\n",
    "            return self\n",
    "        else:\n",
    "            super().init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练算法模型\n",
    "\n",
    "编写 `src.modules.TrainModel` 类来管理损失函数。分类模型使用交叉熵函数来优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from src.modules import NeuralNetwork, TrainModel\n",
    "\n",
    "\n",
    "class TrainClassifyMammals(TrainModel):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    _loss_weights = {\"cross-entropy\": 1.}\n",
    "    \n",
    "    def compute_loss(self, network: NeuralNetwork, batch: Data) -> dict:\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"label\"]\n",
    "        \n",
    "        logits = network(images)\n",
    "        \n",
    "        return {\n",
    "            \"cross-entropy\": self.loss_fn(logits, labels)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练器与插件\n",
    "\n",
    "超参数：\n",
    "- 训练轮数：20\n",
    "- 批次大小：512\n",
    "- 梯度累计：2\n",
    "- 初始学习率：0.01\n",
    "- 初始随机种子：0\n",
    "- 网络结构：resnet18，预训练权重，骨干权重学习率为分类头权重的 1/10\n",
    "\n",
    "拓展插件：\n",
    "- `InitializeNetworkPlugin`：随机初始化网络权重\n",
    "- `SaveCheckpointPlugin`：保存检查点\n",
    "- `EvaluatePlugin`：epoch末尾评估模型，需要定义 `EvaluateModel`\n",
    "- `AdjustLearningRatePlugin`：分别在第 6、16 个 epoch 处调整 lr 为 1/10\n",
    "- `LossLoggerPlugin`，`MetricLoggerPlugin`，`LearningRateLoggerPlugin`\n",
    "- `ProgressBarPlugin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.modules import EvaluateModel, NeuralNetwork\n",
    "from src.utils import move_batch\n",
    "\n",
    "# 评估模型类\n",
    "class EvaluateClassifyMammalsTest(EvaluateModel):\n",
    "    def __init__(self, root, batch_size, device=\"cpu\") -> None:\n",
    "        self._metrics = [\"accuracy\"]\n",
    "        self.dataset = MammalsDataset(root, train=False)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(device)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_batch_labels(self, network, batch_images):\n",
    "        network.eval()\n",
    "        logits = network(batch_images)\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        return pred\n",
    "    \n",
    "    def evaluate(self, network: NeuralNetwork) -> dict[str, float]:\n",
    "        network.to(self.device)\n",
    "        correctness = 0\n",
    "        for batch in DataLoader(self.dataset, batch_size=self.batch_size, num_workers=4):\n",
    "            batch = move_batch(batch, self.device)\n",
    "            images = batch[\"image\"]\n",
    "            labels = batch[\"label\"]\n",
    "            \n",
    "            pred = self.predict_batch_labels(network, images)\n",
    "            correctness += (pred == labels).sum().item()\n",
    "        return {\"accuracy\": correctness / len(self.dataset)}\n",
    "\n",
    "# 学习率调整函数\n",
    "def lr_adjust_fn(lr, epoch, index):\n",
    "    if epoch == 5 + 1 or epoch == 15 + 1:\n",
    "        return lr / 10\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp-1\n",
    "from pathlib import Path\n",
    "from src.modules import Trainer \n",
    "from torch.optim import Adam\n",
    "from src.plugins import (\n",
    "    Plugin,\n",
    "    LoadCheckpointPlugin,\n",
    "    InitializeNetworkPlugin,\n",
    "    SaveCheckpointPlugin,\n",
    "    EvaluatePlugin,\n",
    "    AdjustLearningRatePlugin,\n",
    "    LossLoggerPlugin, MetricLoggerPlugin, LearningRateLoggerPlguin,\n",
    "    ProgressBarPlugin\n",
    ")\n",
    "\n",
    "EXP_INDEX = 1\n",
    "\n",
    "# Hyper Parameters\n",
    "DATA_DIR = \"../data/\"\n",
    "NUM_EPOCH = 20\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "SEED = 0\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "EVAL_PERIOD = 2\n",
    "\n",
    "OUTPUT_DIR = Path(\"../OUTPUTs/mammals\").absolute()\n",
    "LOG_DIR = OUTPUT_DIR / \"log\" / f\"exp-{EXP_INDEX}\"\n",
    "LOG_PERIOD = 5\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoint\" / f\"exp-{EXP_INDEX}\"\n",
    "CHECKPOINT_PERIOD = EVAL_PERIOD\n",
    "\n",
    "# modules\n",
    "\n",
    "# 训练数据集\n",
    "dataset = MammalsDataset(root=DATA_DIR)\n",
    "\n",
    "# 神经网络\n",
    "network = ResNet(\"resnet18\", use_pretrained=True, freeze_backbone=False)\n",
    "\n",
    "# 优化器，backbone 的学习率为 fc 学习率的 1/10\n",
    "optimizer = Adam([\n",
    "    {\"params\": network.backbone_parameters(), \"lr\": LEARNING_RATE / 10},\n",
    "    {\"params\": network.fc_parameters(), \"lr\": LEARNING_RATE}\n",
    "])\n",
    "\n",
    "# 训练和评估模型\n",
    "train_model = TrainClassifyMammals()\n",
    "eval_model = EvaluateClassifyMammalsTest(root=DATA_DIR, batch_size=BATCH_SIZE, device=DEVICE)\n",
    "\n",
    "# 训练器\n",
    "trainer = (\n",
    "    Trainer(\n",
    "        train_model,\n",
    "        num_epochs=NUM_EPOCH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_step=2,\n",
    "        init_seed=SEED,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    # .add_plugin(LoadCheckpointPlugin(\"../OUTPUTs/mammals/checkpoint/exp-8/epoch-6/\"))\n",
    "    .add_plugin(InitializeNetworkPlugin())\n",
    "    .add_plugin(AdjustLearningRatePlugin(lr_adjust_fn))\n",
    "    .add_plugin(EvaluatePlugin(eval_model, eval_period=EVAL_PERIOD))\n",
    "    .add_plugin(SaveCheckpointPlugin(saving_dir=CHECKPOINT_DIR, saving_period=CHECKPOINT_PERIOD))\n",
    "    .add_plugin(LearningRateLoggerPlguin(log_dir=LOG_DIR, log_period=LOG_PERIOD))\n",
    "    .add_plugin(LossLoggerPlugin(log_dir=LOG_DIR, log_period=LOG_PERIOD))\n",
    "    .add_plugin(MetricLoggerPlugin(log_dir=LOG_DIR, log_period=EVAL_PERIOD))\n",
    "    .add_plugin(ProgressBarPlugin())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.loop(dataset, network, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 查看训练结果（TensorBoard）\n",
    "\n",
    "在 CLI 启动 tensorboard\n",
    "```bash\n",
    "cd ~/pytorch-training-framework\n",
    "tensorboard --logdir OUTPUTs/mammals/log\n",
    "```\n",
    "\n",
    "## 7. 使用模型进行推理\n",
    "\n",
    "随机读取一张图片并使用训练后的模型进行判断其物种。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "\n",
    "NETWORK_WEIGHTS_PATH = \"../OUTPUTs/mammals/checkpoint/exp-1/epoch-18/network_state_dict.pth\"\n",
    "\n",
    "network.load_state_dict(torch.load(NETWORK_WEIGHTS_PATH))\n",
    "eval_dataset = MammalsDataset(root=\"../data/\", train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 随机选择 eval_dataset 中的图片做推断\n",
    "index = random.randint(0, len(eval_dataset)-1)\n",
    "\n",
    "data = eval_dataset.get_image_label(index)\n",
    "image = data[\"image\"]\n",
    "img_tensor = eval_dataset[index][\"image\"].unsqueeze_(0)\n",
    "label = data[\"label\"]\n",
    "\n",
    "\n",
    "print(\"true label:\", label)\n",
    "pred = eval_dataset.labels[eval_model.predict_batch_labels(network, img_tensor).tolist()[0]]\n",
    "print(\"pred label:\", pred)\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
